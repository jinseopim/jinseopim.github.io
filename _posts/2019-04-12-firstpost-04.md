---
layout: post
title:  "[빅데이터 아키텍처 구축] (08) Python 에서 개발하기"
date:   2019-04-13 05:37:23 +0700
categories: [python, codefights]
---

___

__[목차]__

- [(01) 빅데이터 분석 및 서비스 아키텍처 설명](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-02.html)
- [(02) 아키텍처 구성 (수집/개발 도구 설치) : Python](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-03-00.html)
- [(03) 아키텍처 구성 (수집/개발 도구 설치) : R/RStudio](https://jinseopim.github.io/r/rstudio/2019/04/12/firstpost-03-01.html)
- [(04) 아키텍처 구성 (수집/개발 도구 설치) : Node.js/VSCode](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-03-02.html)
- [(05) 아키텍처 구성 (저장/분석 도구 설치) : Docker, elastic](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-03-03.html)
- [(06) RStudio 에서 개발하기 : ElasticSearch](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-03-04.html)
- [(07) Node.js 에서 개발하기 : Dialogflow](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-03-05.html)
- __[(08) Python 에서 개발하기](https://jinseopim.github.io/python/codefights/2019/04/12/firstpost-04.html)__

___

## (08) Python 에서 개발하기
###

KoNLPy와 Word2Vec를 이용한 관심 분류별 감성분석 입니다.

아래 두 사이트를 참고하여 구현하였습니다.
- [[Keras] KoNLPy를 이용한 한국어 영화 리뷰 감정 분석](https://cyc1am3n.github.io/2018/11/10/classifying_korean_movie_review.html)
  - 전체코드 : http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-Extra-classifying_korean_movie_review.ipynb
  - 참고 : http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-4-classifying_movie_reviews.ipynb
  - KoNLPy : https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/
- [한글을 이용한 데이터마이닝및 word2vec이용한 유사도 분석](http://blog.naver.com/PostView.nhn?blogId=2feelus&logNo=220384206922&redirect=Dlog&widgetTypeCall=true)

Python3.7 에서 테스트 된 소스이며, Python3.7 설치 및 환경설정이 완료 되었다는 가정하에 진행 합니다.
Python 설치 및 환경설정은 좋은 자료가 많이 있습니다.

- Python 설치 참고
  - [https://wikidocs.net/8](https://wikidocs.net/8)

감성분석을 위해 필요한 라이브러리를 설치하고 로드합니다.

- 필요 라이브러리
  - nltk : http://www.nltk.org/data.html
  - numpy : pip install numpy
  - konlpy : pip install konlpy

- 필요 데이터
  - [Naver sentiment movie corpus](https://github.com/e9t/nsmc/)에서 ratings_train.txt 와 ratings_test.txt 를 다운로드 합니다.

아래는 전체 소스 입니다.
자세한 내용은 주석을 참조하시면 되겠습니다.

```Python
from konlpy.tag import Okt
import json
import os
from pprint import pprint
import nltk
import numpy as np

# 파일 로드를 위한 함수
def read_data(filename):
    with open(filename, 'r', encoding='UTF8') as f:
        data = [line.split('\t') for line in f.read().splitlines()]
        # txt 파일의 헤더(id document label)는 제외하기
        data = data[1:]
    return data

train_data = read_data('ratings_train.txt')
test_data = read_data('ratings_test.txt')

print(len(train_data))
print(len(train_data[0]))
print(len(test_data))
print(len(test_data[0]))

# 1) morphs : 형태소 추출
# 2) pos : 품사 부착(Part-of-speech tagging)
# 3) nouns : 명사 추출
okt = Okt()

# 테스트
print(okt.pos(u'이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요'))

def tokenize(doc):
    # norm은 정규화, stem은 근어로 표시하기를 나타냄
    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]

if os.path.isfile('train_docs.json'):
    with open('train_docs.json', encoding='UTF8') as f:
        train_docs = json.load(f)
    with open('test_docs.json', encoding='UTF8') as f:
        test_docs = json.load(f)
else:
    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]
    # JSON 파일로 저장
    with open('train_docs.json', 'w', encoding="utf-8") as make_file:
        json.dump(train_docs, make_file, ensure_ascii=False, indent="\t")
    with open('test_docs.json', 'w', encoding="utf-8") as make_file:
        json.dump(test_docs, make_file, ensure_ascii=False, indent="\t")

# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용
pprint(train_docs[0])

tokens = [t for d in train_docs for t in d[0]]
print(len(tokens))

text = nltk.Text(tokens, name='NMSC')

# 전체 토큰의 개수
print(len(text.tokens))

# 중복을 제외한 토큰의 개수
print(len(set(text.tokens)))

# 출현 빈도가 높은 상위 토큰 10개
pprint(text.vocab().most_common(10))

# 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.
# most_common(100) 의 수를 높일 수록 정확도가 올라갑니다.
selected_words = [f[0] for f in text.vocab().most_common(100)]

selected_words

def term_frequency(doc):
    return [doc.count(word) for word in selected_words]

train_x = [term_frequency(d) for d, _ in train_docs]
test_x = [term_frequency(d) for d, _ in test_docs]
train_y = [c for _, c in train_docs]
test_y = [c for _, c in test_docs]

x_train = np.asarray(train_x).astype('float32')
x_test = np.asarray(test_x).astype('float32')
y_train = np.asarray(train_y).astype('float32')
y_test = np.asarray(test_y).astype('float32')


model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(100,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])

model.fit(x_train, y_train, epochs=10, batch_size=512)
results = model.evaluate(x_test, y_test)


def predict_pos_neg(review):
    token = tokenize(review)
    tf = term_frequency(token)
    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)
    score = float(model.predict(data))
    if(score > 0.5):
        print("[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n".format(review, score * 100))
    else:
        print("[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n".format(review, (1 - score) * 100))


# 테스트
predict_pos_neg("올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.")
predict_pos_neg("배경 음악이 영화의 분위기랑 너무 안 맞았습니다. 몰입에 방해가 됩니다.")
predict_pos_neg("주연 배우가 신인인데 연기를 진짜 잘 하네요. 몰입감 ㅎㄷㄷ")
predict_pos_neg("믿고 보는 감독이지만 이번에는 아니네요")
predict_pos_neg("주연배우 때문에 봤어요")



# http://blog.naver.com/PostView.nhn?blogId=2feelus&logNo=220384206922&redirect=Dlog&widgetTypeCall=true
# [출처] 한글을 이용한 데이터마이닝및 word2vec이용한 유사도 분석|작성자 IDEO (참고하여 소스 커스터마이징)
# 1. 읽기
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from konlpy.corpus import kobill    # Docs from pokr.kr/bill
files_ko = kobill.fileids()         # Get file ids

# ratings_train.txt 는  konlpy의 corpus아래에 있는 kobill directory에 미리 저장되어있어야 한다.
# /Library/Python/2.7/site-packages/konlpy/data/corpus/kobill
doc_ko = kobill.open('ratings_train.txt').read()

# 2.Tokenize (의미단어 검출)
from konlpy.tag import Okt
import os
import json

# 학습시간이 오래 걸리므로 파일로 저장하여 처리 한다.
if os.path.isfile('tokens_ko_morphs.txt'):
    with open('tokens_ko_morphs.txt', encoding='UTF8') as f:
        tokens_ko_morphs = json.load(f)
else:
    okt = Okt()
    tokens_ko_morphs = okt.morphs(doc_ko)

    # JSON 파일로 저장
    with open('tokens_ko_morphs.txt', 'w', encoding="utf-8") as make_file:
        json.dump(tokens_ko_morphs, make_file, ensure_ascii=False, indent="\t")

# 3. Token Wapper 클래스 만들기(token에대해 이런 저런 처리를 하기 위해)
import nltk

# 일반적인 영화 평론의 기준과 유사한 단어를 추출한다.
# 스토리, 영상, 연출, 연기
# ko = nltk.Text(tokens_ko_morphs, name='연기')   # 이름은 아무거나
ko = nltk.Text(tokens_ko_morphs)

# 4. 토근 정보및 단일 토큰 정보 알아내기
print(len(ko.tokens))       # returns number of tokens (document length)
print(len(set(ko.tokens)))  # returns number of unique tokens
ko.vocab()                  # returns frequency distribution

# 5. 챠트로 보기
ko.plot(50) #상위 50개의 unique token에 대해서 plot한결과를 아래와 같이 보여준다.

# 6. 특정 단어에 대해서 빈도수 확인하기
print(ko.count(str('스토리')))
print(ko.count(str('영상')))
print(ko.count(str('연출')))
print(ko.count(str('연기')))

# 7. 분산 차트 보기 (dispersion plot)
ko.dispersion_plot(['스토리','영상','연출','연기'])

# 8. 색인하기(본문속에서 해당 검색어가 포함된 문장을 찾아주는 것)
story_tmp = ko.concordance_list(('스토리'))
image_tmp = ko.concordance_list(('영상'))
direct_tmp = ko.concordance_list(('연출'))
act_tmp = ko.concordance_list(('연기'))

# 스토리 관련 문장을 추출한다.
str_story_tmp = ""
for i, v in enumerate(story_tmp):
    print("story_tmp[", i, "][4]", story_tmp[i][4])
    str_story_tmp = str_story_tmp + story_tmp[i][4]

str_story_tmp

# 영상 관련 문장을 추출한다.
str_image_tmp = ""
for i, v in enumerate(image_tmp):
    print("story_tmp[", i, "][4]", image_tmp[i][4])
    str_image_tmp = str_image_tmp + image_tmp[i][4]

str_image_tmp


# 연출 관련 문장을 추출한다.
str_direct_tmp = ""
for i, v in enumerate(direct_tmp):
    print("story_tmp[", i, "][4]", direct_tmp[i][4])
    str_direct_tmp = str_image_tmp + direct_tmp[i][4]

str_direct_tmp


# 연기 관련 문장을 추출한다.
str_act_tmp = ""
for i, v in enumerate(act_tmp):
    print("story_tmp[", i, "][4]", act_tmp[i][4])
    str_act_tmp = str_image_tmp + act_tmp[i][4]

str_act_tmp

predict_pos_neg(str_story_tmp)
predict_pos_neg(str_image_tmp)
predict_pos_neg(str_direct_tmp)
predict_pos_neg(str_act_tmp)

```

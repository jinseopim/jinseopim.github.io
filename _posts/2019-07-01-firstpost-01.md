---
layout: post
title:  "[Dialogflow] 기계학습을 이용한 영화감성분석 응용하기"
date:   2019-07-04 05:37:23 +0700
categories: [python, codefights]
---

___

__[목차]__

- [1. Dialogflow 와 Python 을 이용한 챗봇 구현](#-1.-dialogflow-와-python-을-이용한-챗봇-구현)
  - [01. Python 및 Flask 설치](##-02.-Python-및-Flask-설치)
  - [02. PyCharm 설치](##-01.-pyCharm-설치)
  - [03. 전체 Event 흐름도](##-03.-전체-Event-흐름도)
  - [04. Flask 란?](##-04.-Flask-란?)
  - [05. Webhook 이란?](##-05.-Webhook-이란?)
  - [06. Dialogflow 란?](##-06.-Dialogflow-란?)
  - [07. Python Flask Webhook 서버 만들기](##-07.-Python-Flask-Webhook-서버-만들기)
  - [08. Local 컴퓨터에서 Public 서비스 하기](##-08.-Local-컴퓨터에서-Public-서비스-하기)
  - [09. Dialogflow 에서 Webhook 설정하기](##-09.-Dialogflow-에서-Webhook-설정하기)
  - [10. Dialogflow Intent 작성](##-10.-Dialogflow-Intent-작성)
  - [11. Dialogflow Intent Parameter 설정](##-11.-Dialogflow-Intent-Parameter-설정)
  - [12. Dialogflow Webhook 응답확인](##-12.-dialogflow-webhook-응답확인)
- [2. KoNLPy와 Word2Vec를 이용한 관심 영화 분류별 감성분석](#-2.-KoNLPy와-Word2Vec를-이용한-관심-영화-분류별-감성분석)
  - [01. Naver sentiment movie corpus 를 이용한 감성분석](01.-Naver-sentiment-movie-corpus를-이용한-감성분석)
  - [02. word2vec 유사도 분석을 이용한 분류별 점수 산정](02.-word2vec-유사도-분석을-이용한-분류별-점수-산정)
- [3. 감성분석 결과를 Dialogflow 로 응답하기](3.-감성분석-결과를-Dialogflow로-응답하기)

___


# 1. Dialogflow 와 Python 을 이용한 챗봇 구현
## 01. Python 및 Flask 설치
### Python : [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)
**1) OS에 해당하는 설치 파일 다운로드**
![Imgur](https://i.imgur.com/ShHwZyq.png)
**2) 환경변수 추가**
- 탐색기 -> 내 PC(내 컴퓨터) -> 오른쪽 마우스 클릭 -> 속성 -> 고급 시스템 설정 -> 환경 변수 -> 시스템 변수 -> Path -> 편집
- 아래 세개의 환경변수를 추가.
\Anaconda3
\Anaconda3\Scripts
\Anaconda3\Library\bin
	- 탐색기 -> 내 PC(내 컴퓨터) -> 오른쪽 마우스 클릭 -> 속성
![Imgur](https://i.imgur.com/PDdOwyx.png)
	- 고급 시스템 설정
![Imgur](https://i.imgur.com/nxem2Pm.png)
	- 환경 변수 -> 시스템 변수
![Imgur](https://i.imgur.com/Jx6NwY1.png)
	- Path 편집
![Imgur](https://i.imgur.com/1yGgRpJ.png)

**3) Python 설치확인**
```Console
C:\>python --version
Python 3.7.3
C:\>pip --version
pip 19.0.3 from C:\Users\user\Anaconda3\lib\site-packages\pip (python 3.7)
```

**4) Python Flask 설치**
```Console
C:\>pip install flask
Requirement already satisfied: flask in c:\users\user\anaconda3\lib\site-packages (1.0.2)
Requirement already satisfied: itsdangerous>=0.24 in c:\users\user\anaconda3\lib\site-packages (from flask) (1.1.0)
Requirement already satisfied: Werkzeug>=0.14 in c:\users\user\anaconda3\lib\site-packages (from flask) (0.14.1)
Requirement already satisfied: Jinja2>=2.10 in c:\users\user\anaconda3\lib\site-packages (from flask) (2.10)
Requirement already satisfied: click>=5.1 in c:\users\user\anaconda3\lib\site-packages (from flask) (7.0)
Requirement already satisfied: MarkupSafe>=0.23 in c:\users\user\anaconda3\lib\site-packages (from Jinja2>=2.10->flask) (1.1.1)
```

## 02. PyCharm 설치
### - 설명이 잘되어 있는 곳 : [파이참(PyCharm) 설치하기-김중운](https://wikidocs.net/20343)

## 03. 전체 Event 흐름도
![Imgur](https://i.imgur.com/EZqG36o.png)
[출처] https://dialogflow.com/docs/events/custom-events (수정)

## 04. Flask 란?
- 플라스크(Flask)는 파이썬으로 작성된 마이크로 웹 프레임워크이다.
- 플라스크는 특별한 도구나 라이브러리가 필요 없기 때문에 마이크로 프레임워크라 부른다.
- 마이크로 + 프레임워크 = 마이크로(필요한 기능만) + 프레임워크(뼈대)
- 아래는 가장 단순화된 소스코드 이다.
- app.py 로 저장 후 python app.py 로 실행

```Console
from flask import Flask
app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello World!"

if __name__ == "__main__":
    app.run()
```

## 05. Webhook 이란?
![Imgur](https://developers.exlibrisgroup.com/wp-content/uploads/2018/12/webhooks.png)
[출처] https://developers.exlibrisgroup.com/alma/integrations/webhooks/
- Hooking : 어떠한 액션 앞 또는 뒤에 추가로 어떠한 일을 하도록 하는 것
- Webhook : 웹 서비스를 제공해주는 서버 측에서 어떠한 이벤트(또는 데이터)를 외부에 전달하는 방법 중 하나

## 06. Dialogflow 란?

### Dialogflow
- Dialogflow (구)api.ai, Speaktoit : Google 기계학습 기반의 챗봇 플랫폼
- 화자의 의도, 속성, 문맥을 분석하여 적절한 응답을 보냄.

#### 1) 계정 생성 및 로그인
![Imgur](https://i.imgur.com/CCXyMCz.png)

- ① 계정생성
- ② 계정생성 후 Dialogflow Console 이동

#### 2) 핵심구성
  - 의도 : Intent
  - 속성 : Entity
  - 문맥 : Context

#### 3) 주요메뉴
![Imgur](https://i.imgur.com/aaXGwd9.png)
  - Intent : 수리가 가능할까요? -> 어느날로 예약을 하시겠습니까?
  - Entity : 내일 오후 2시 되나요 -> '내일', '오후 2시'
  - Fulfillment : Web Hook 처리 지원
  - Context : 무엇을 위한 내일 오후 2시 인가를 이해
  - Event : 이벤트 발생 시 원하는 intent 로 이동
  - Slot Filling : 빈 슬롯을 채워넣는 것처럼 대화를 진행(반드시 필요한 Entity 를 쉽게 사용자로부터 받음)
  - Training(재학습) : intent 매핑 가능

## 07. Python Flask Webhook 서버 만들기
- 소스코드


```Console
from flask import Flask
app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello World!"

if __name__ == "__main__":
    app.run()
```

- 실행 : python app.py
- http://127.0.0.1:5000 에서 실행여부 확인

## 08. Local 컴퓨터에서 Public 서비스 하기
### Ngrok 실행하기
- 다운로드 : ngrok.io

```Console
ngrok http 5000
```

![Imgur](https://i.imgur.com/mSdSpu7.png)

## 09. Dialogflow 에서 Webhook 설정하기
- Dialogflow에서 webhook 설정
![Imgur](https://i.imgur.com/djTurXA.png)
  - Webhook 을 ENABLE 시킨다.
  - ngrok을 실행해서 얻어진 Forwarding Url 을 Dialogflow Fulfillment URL에 입력한다.

- Dialogflow Intent에서 Webhook 활성화
![Imgur](https://i.imgur.com/U4H95Ue.png)
  - Webhook을 적용시킬 Intent에서 Fulfillment를 활성화 시킨다.

## 10. Dialogflow Intent 작성
- Training Phrases는 특정 의도를 식별 및 호출하는 문장.
![Imgur](https://i.imgur.com/RWv7XzS.png?1)

## 11. Dialogflow Intent Parameter 설정
1) Training phrase에 의도를 식별할 문장을 입력한다.
![Imgur](https://i.imgur.com/AVA0k7z.png)

2) "ramen" 을 왼쪽 클릭하여 드래그 한 후 Pop-up 메뉴에서 @sys.any 선택
![Imgur](https://i.imgur.com/hJA1mBx.png)

3) PARAMETER NAME을 food 로 변경한다.
![Imgur](https://i.imgur.com/paaca5Y.png)
- 요청 처리를 위해 웹 훅에서 사용할 수있는 "작업 및 매개 변수"를 설정.
- 매개 변수를 webhook에 전달하여 원하는 응답을 생성 가능.

## 12. Dialogflow Webhook 응답확인
- 04에서 만들었던 app.py 를 아래와 같이 수정.

```Console
# import Flask
from flask import Flask, request, make_response, jsonify
import json
from collections import OrderedDict

# flask app 초기화
app = Flask(__name__)

# 기본 route
@app.route('/')
def index():
    return 'Hello World!'

# webhook route
@app.route('/webhook', methods=['GET', 'POST'])
def webhook():
    # return response
    return make_response(jsonify(results()))

# 응답 함수
def results():
    # build a request object
    data = request.get_json(silent=True)
    food = data['queryResult']['parameters']['food']

    file_data = OrderedDict()
    file_data["fulfillmentText"] = "Did you order a " + food + "?"

    return file_data

# app 실행
if __name__ == '__main__':
   app.run()
```

- flask 실행
```Console
python app.py
```

- 응답 확인
![Imgur](https://i.imgur.com/1jJewrl.png)
- ① webhook에서 생성한 응답결과 확인
- ② webhook에서 응답이 왔다는 내용 확인


# 2. KoNLPy와 Word2Vec를 이용한 관심 영화 분류별 감성분석
## 01. Naver sentiment movie corpus 를 이용한 감성분석
감성분석을 위해 필요한 라이브러리를 설치하고 로드합니다.

#### 1) 필요 라이브러리
- nltk : http://www.nltk.org/data.html
- numpy : pip install numpy
- konlpy : pip install konlpy

#### 2) 필요 데이터
- [Naver sentiment movie corpus](https://github.com/e9t/nsmc/)에서 ratings_train.txt 와 ratings_test.txt 를 다운로드 합니다.

#### 3) 감성분석 소스코드
```Console
from konlpy.tag import Okt
import json
import os
from pprint import pprint
import nltk
import numpy as np

# 파일 로드를 위한 함수
def read_data(filename):
    with open(filename, 'r', encoding='UTF8') as f:
        data = [line.split('\t') for line in f.read().splitlines()]
        # txt 파일의 헤더(id document label)는 제외하기
        data = data[1:]
    return data

train_data = read_data('ratings_train.txt')
test_data = read_data('ratings_test.txt')

print(len(train_data))
print(len(train_data[0]))
print(len(test_data))
print(len(test_data[0]))

# 1) morphs : 형태소 추출
# 2) pos : 품사 부착(Part-of-speech tagging)
# 3) nouns : 명사 추출
okt = Okt()

# 테스트
print(okt.pos(u'이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요'))

def tokenize(doc):
    # norm은 정규화, stem은 근어로 표시하기를 나타냄
    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]

if os.path.isfile('train_docs.json'):
    with open('train_docs.json', encoding='UTF8') as f:
        train_docs = json.load(f)
    with open('test_docs.json', encoding='UTF8') as f:
        test_docs = json.load(f)
else:
    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]
    # JSON 파일로 저장
    with open('train_docs.json', 'w', encoding="utf-8") as make_file:
        json.dump(train_docs, make_file, ensure_ascii=False, indent="\t")
    with open('test_docs.json', 'w', encoding="utf-8") as make_file:
        json.dump(test_docs, make_file, ensure_ascii=False, indent="\t")

# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용
pprint(train_docs[0])

tokens = [t for d in train_docs for t in d[0]]
print(len(tokens))

text = nltk.Text(tokens, name='NMSC')

# 전체 토큰의 개수
print(len(text.tokens))

# 중복을 제외한 토큰의 개수
print(len(set(text.tokens)))

# 출현 빈도가 높은 상위 토큰 10개
pprint(text.vocab().most_common(10))

# 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.
# most_common(100) 의 수를 높일 수록 정확도가 올라갑니다.
selected_words = [f[0] for f in text.vocab().most_common(100)]

selected_words

def term_frequency(doc):
    return [doc.count(word) for word in selected_words]

train_x = [term_frequency(d) for d, _ in train_docs]
test_x = [term_frequency(d) for d, _ in test_docs]
train_y = [c for _, c in train_docs]
test_y = [c for _, c in test_docs]

x_train = np.asarray(train_x).astype('float32')
x_test = np.asarray(test_x).astype('float32')
y_train = np.asarray(train_y).astype('float32')
y_test = np.asarray(test_y).astype('float32')


model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(100,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])

model.fit(x_train, y_train, epochs=10, batch_size=512)
results = model.evaluate(x_test, y_test)


def predict_pos_neg(review):
    token = tokenize(review)
    tf = term_frequency(token)
    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)
    score = float(model.predict(data))
    if(score > 0.5):
        print("[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n".format(review, score * 100))
    else:
        print("[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n".format(review, (1 - score) * 100))


# 테스트
predict_pos_neg("올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.")
predict_pos_neg("배경 음악이 영화의 분위기랑 너무 안 맞았습니다. 몰입에 방해가 됩니다.")
predict_pos_neg("주연 배우가 신인인데 연기를 진짜 잘 하네요. 몰입감 ㅎㄷㄷ")
predict_pos_neg("믿고 보는 감독이지만 이번에는 아니네요")
predict_pos_neg("주연배우 때문에 봤어요")
```


## 02. word2vec 유사도 분석을 이용한 분류별 점수 산정
```Console
# 1. 읽기
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from konlpy.corpus import kobill    # Docs from pokr.kr/bill
files_ko = kobill.fileids()         # Get file ids

# ratings_train.txt 는  konlpy의 corpus아래에 있는 kobill directory에 미리 저장되어있어야 한다.
# /Library/Python/2.7/site-packages/konlpy/data/corpus/kobill
doc_ko = kobill.open('ratings_train.txt').read()

# 2.Tokenize (의미단어 검출)
from konlpy.tag import Okt
import os
import json

# 학습시간이 오래 걸리므로 파일로 저장하여 처리 한다.
if os.path.isfile('tokens_ko_morphs.txt'):
    with open('tokens_ko_morphs.txt', encoding='UTF8') as f:
        tokens_ko_morphs = json.load(f)
else:
    okt = Okt()
    tokens_ko_morphs = okt.morphs(doc_ko)

    # JSON 파일로 저장
    with open('tokens_ko_morphs.txt', 'w', encoding="utf-8") as make_file:
        json.dump(tokens_ko_morphs, make_file, ensure_ascii=False, indent="\t")

# 3. Token Wapper 클래스 만들기(token에대해 이런 저런 처리를 하기 위해)
import nltk

# 일반적인 영화 평론의 기준과 유사한 단어를 추출한다.
# 스토리, 영상, 연출, 연기
# ko = nltk.Text(tokens_ko_morphs, name='연기')   # 이름은 아무거나
ko = nltk.Text(tokens_ko_morphs)

# 4. 토근 정보및 단일 토큰 정보 알아내기
print(len(ko.tokens))       # returns number of tokens (document length)
print(len(set(ko.tokens)))  # returns number of unique tokens
ko.vocab()                  # returns frequency distribution

# 5. 챠트로 보기
ko.plot(50) #상위 50개의 unique token에 대해서 plot한결과를 아래와 같이 보여준다.

# 6. 특정 단어에 대해서 빈도수 확인하기
print(ko.count(str('스토리')))
print(ko.count(str('영상')))
print(ko.count(str('연출')))
print(ko.count(str('연기')))

# 7. 분산 차트 보기 (dispersion plot)
ko.dispersion_plot(['스토리','영상','연출','연기'])

# 8. 색인하기(본문속에서 해당 검색어가 포함된 문장을 찾아주는 것)
story_tmp = ko.concordance_list(('스토리'))
image_tmp = ko.concordance_list(('영상'))
direct_tmp = ko.concordance_list(('연출'))
act_tmp = ko.concordance_list(('연기'))

# 스토리 관련 문장을 추출한다.
str_story_tmp = ""
for i, v in enumerate(story_tmp):
    print("story_tmp[", i, "][4]", story_tmp[i][4])
    str_story_tmp = str_story_tmp + story_tmp[i][4]

str_story_tmp

# 영상 관련 문장을 추출한다.
str_image_tmp = ""
for i, v in enumerate(image_tmp):
    print("story_tmp[", i, "][4]", image_tmp[i][4])
    str_image_tmp = str_image_tmp + image_tmp[i][4]

str_image_tmp


# 연출 관련 문장을 추출한다.
str_direct_tmp = ""
for i, v in enumerate(direct_tmp):
    print("story_tmp[", i, "][4]", direct_tmp[i][4])
    str_direct_tmp = str_image_tmp + direct_tmp[i][4]

str_direct_tmp


# 연기 관련 문장을 추출한다.
str_act_tmp = ""
for i, v in enumerate(act_tmp):
    print("story_tmp[", i, "][4]", act_tmp[i][4])
    str_act_tmp = str_image_tmp + act_tmp[i][4]

str_act_tmp

predict_pos_neg(str_story_tmp)
predict_pos_neg(str_image_tmp)
predict_pos_neg(str_direct_tmp)
predict_pos_neg(str_act_tmp)
```

위 소스는 아래 두 사이트를 참고하여 구현하였습니다.
- [[Keras] KoNLPy를 이용한 한국어 영화 리뷰 감정 분석](https://cyc1am3n.github.io/2018/11/10/classifying_korean_movie_review.html)
  - 전체코드 : http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-Extra-classifying_korean_movie_review.ipynb
  - 참고 : http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-4-classifying_movie_reviews.ipynb
  - KoNLPy : https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/
- [한글을 이용한 데이터마이닝및 word2vec이용한 유사도 분석](http://blog.naver.com/PostView.nhn?blogId=2feelus&logNo=220384206922&redirect=Dlog&widgetTypeCall=true)

# 3. 감성분석 결과를 Dialogflow로 응답하기
##### 1) 프로젝트 폴더 구조
![Imgur](https://i.imgur.com/INS7vJt.png)

##### 2) app.py (Flask 실행 및 Webhook 구현 소스)
```Console
# import Flask
from flask import Flask, request, make_response, jsonify, render_template
import json
from collections import OrderedDict

import os
import requests

from library.naver_movie_api import getInfoFromNaver, get_movie_review
from library.naver_movie_code_api import get_movie_code
from library.movie_senti_anal import predict_pos_neg

# flask app 초기화
app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

# webhook route
@app.route('/webhook', methods=['GET', 'POST'])
def webhook():
    # return response
    return make_response(jsonify(results()))

@app.route('/get_movie_detail', methods=['POST'])
def get_movie_detail():
    data = request.get_json(silent=True)

    try:
        movie = data['queryResult']['parameters']['movie']

        # api_key = os.getenv('OMDB_API_KEY')
        # movie_detail = requests.get('http://www.omdbapi.com/?t={0}&apikey={1}'.format(movie, api_key)).content

        # movie_detail = json.loads(movie_detail)
        print(movie)
        print(getInfoFromNaver(movie), type(getInfoFromNaver(movie)))
        print(get_movie_code(movie))

        movie_code = get_movie_code(movie)
        print(movie_code, type(movie_code))
        #print(get_movie_review)

        print(''.join(movie_code))

        print(get_movie_review(''.join(movie_code)))

        print(predict_pos_neg(get_movie_review(''.join(movie_code))))


        # response = """
        #     Poster : {0}
        #     Title : {1}
        #     Released: {2}
        #     Actors: {3}
        #     Plot: {4}
        # """.format(movie_detail['Poster'], movie_detail['Title'], movie_detail['Released'], movie_detail['Actors'],
        #            movie_detail['Plot'])
    except:
        response = "Error, please try again"

    # reply = {"fulfillmentText": response}

    # print(reply)

    # return make_response(jsonify(reply))
    return make_response(jsonify(movie))

# 응답 함수
def results():
    # build a request object
    data = request.get_json(silent=True)
    food = data['queryResult']['parameters']['food']

    file_data = OrderedDict()
    file_data["fulfillmentText"] = "Did you order a " + food + "?"

    return file_data

# app 실행
if __name__ == '__main__':
   app.run()
```
##### 3) library
#####- movie_senti_anal.py : 감성분석 라이브러리
#####- movie_word2vec.py : Word2vec 라이브러리
#####- naver_movie_api.py : 네이버 영화 정보 크롤러
#####- naver_movie_code_api.py : 네이버 영화 코드 크롤러
#####- df_response_lib.py : Webhook 라이브러리

- movie_senti_anal.py : 감성분석 라이브러리
```Console
# https://cyc1am3n.github.io/2018/11/10/classifying_korean_movie_review.html
# KoNLPy, nltk, Keras를 이용해서 한국어 영화 리뷰의 감정을 분석
# 데이터셋은 Naver sentiment movie corpus (http://github.com/e9t/nsmc/)
# 전체코드 : http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-Extra-classifying_korean_movie_review.ipynb
# 참고 : http://nbviewer.jupyter.org/github/cyc1am3n/Deep-Learning-with-Python/blob/master/Chap03-getting_started_with_neural_networks/Chap03-4-classifying_movie_reviews.ipynb
# KoNLPy : https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/
from konlpy.tag import Okt
import json
import os
from pprint import pprint
import nltk
import numpy as np
from tensorflow.keras import models, layers, optimizers, losses, metrics
from gensim.models import Word2Vec

import re
from lxml import etree
from nltk.tokenize import word_tokenize, sent_tokenize

import matplotlib.pyplot as plt
from matplotlib import font_manager, rc
import time
import timeit

def read_data(filename):
    with open(filename, 'r', encoding='UTF8') as f:
        data = [line.split('\t') for line in f.read().splitlines()]
        # txt 파일의 헤더(id document label)는 제외하기
        data = data[1:]
    return data

train_data = read_data('C:\\Project\\02.Python\\movie_senti_anal\\input_data\\ratings_train.txt')
test_data = read_data('C:\\Project\\02.Python\\movie_senti_anal\\input_data\\ratings_test.txt')

print(len(train_data))
print(len(train_data[0]))
print(len(test_data))
print(len(test_data[0]))

# 1) morphs : 형태소 추출
# 2) pos : 품사 부착(Part-of-speech tagging)
# 3) nouns : 명사 추출
okt = Okt()
print(okt.pos(u'이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요'))

def tokenize(doc):
    # norm은 정규화, stem은 근어로 표시하기를 나타냄
    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]

if os.path.isfile('train_docs.json'):
    with open('train_docs.json', encoding='UTF8') as f:
        train_docs = json.load(f)
    with open('test_docs.json', encoding='UTF8') as f:
        test_docs = json.load(f)
else:
    train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
    test_docs = [(tokenize(row[1]), row[2]) for row in test_data]
    # JSON 파일로 저장
    with open('train_docs.json', 'w', encoding="utf-8") as make_file:
        json.dump(train_docs, make_file, ensure_ascii=False, indent="\t")
    with open('test_docs.json', 'w', encoding="utf-8") as make_file:
        json.dump(test_docs, make_file, ensure_ascii=False, indent="\t")

# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용
pprint(train_docs[0])

tokens = [t for d in train_docs for t in d[0]]
print(len(tokens))

text = nltk.Text(tokens, name='NMSC')

# 전체 토큰의 개수
print(len(text.tokens))

# 중복을 제외한 토큰의 개수
print(len(set(text.tokens)))

# 출현 빈도가 높은 상위 토큰 10개
pprint(text.vocab().most_common(10))

# 시간이 꽤 걸립니다! 시간을 절약하고 싶으면 most_common의 매개변수를 줄여보세요.
# most_common(100) 의 수를 높일 수록 정확도가 올라갑니다.
selected_words = [f[0] for f in text.vocab().most_common(100)]

selected_words

def term_frequency(doc):
    return [doc.count(word) for word in selected_words]

train_x = [term_frequency(d) for d, _ in train_docs]
test_x = [term_frequency(d) for d, _ in test_docs]
train_y = [c for _, c in train_docs]
test_y = [c for _, c in test_docs]

x_train = np.asarray(train_x).astype('float32')
x_test = np.asarray(test_x).astype('float32')
y_train = np.asarray(train_y).astype('float32')
y_test = np.asarray(test_y).astype('float32')


model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(100,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])

model.fit(x_train, y_train, epochs=10, batch_size=512)
results = model.evaluate(x_test, y_test)


def predict_pos_neg(review):
    token = tokenize(review)
    tf = term_frequency(token)
    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)
    score = float(model.predict(data))
    if(score > 0.5):
        print("[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n".format(review, score * 100))
    else:
        print("[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n".format(review, (1 - score) * 100))


# 테스트
predict_pos_neg("올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.")
predict_pos_neg("배경 음악이 영화의 분위기랑 너무 안 맞았습니다. 몰입에 방해가 됩니다.")
predict_pos_neg("주연 배우가 신인인데 연기를 진짜 잘 하네요. 몰입감 ㅎㄷㄷ")
predict_pos_neg("믿고 보는 감독이지만 이번에는 아니네요")
predict_pos_neg("주연배우 때문에 봤어요")



# http://blog.naver.com/PostView.nhn?blogId=2feelus&logNo=220384206922&redirect=Dlog&widgetTypeCall=true
# [출처] 한글을 이용한 데이터마이닝및 word2vec이용한 유사도 분석|작성자 IDEO (참고하여 소스 커스터마이징)
# 1. 읽기
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from konlpy.corpus import kobill    # Docs from pokr.kr/bill
files_ko = kobill.fileids()         # Get file ids

# news.txt는 http://boilerpipe-web.appspot.com/ 를 통해 포탈뉴스 부분에서 긁어왔다.
# news.txt 는  konlpy의 corpus아래에 있는 kobill directory에 미리 저장되어있어야 한다.
# /Library/Python/2.7/site-packages/konlpy/data/corpus/kobill
doc_ko = kobill.open('ratings_train.txt').read()

# 2.Tokenize (의미단어 검출)
from konlpy.tag import Okt
import os
import json

# 학습시간이 오래 걸리므로 파일로 저장하여 처리 한다.
if os.path.isfile('tokens_ko_morphs.txt'):
    with open('tokens_ko_morphs.txt', encoding='UTF8') as f:
        tokens_ko_morphs = json.load(f)
else:
    okt = Okt()
    tokens_ko_morphs = okt.morphs(doc_ko)

    # JSON 파일로 저장
    with open('tokens_ko_morphs.txt', 'w', encoding="utf-8") as make_file:
        json.dump(tokens_ko_morphs, make_file, ensure_ascii=False, indent="\t")

# 3. Token Wapper 클래스 만들기(token에대해 이런 저런 처리를 하기 위해)
import nltk

# 일반적인 영화 평론의 기준과 유사한 단어를 추출한다.
# 스토리, 영상, 연출, 연기
# ko = nltk.Text(tokens_ko_morphs, name='연기')   # 이름은 아무거나
ko = nltk.Text(tokens_ko_morphs)

# 4. 토근 정보및 단일 토큰 정보 알아내기
print(len(ko.tokens))       # returns number of tokens (document length)
print(len(set(ko.tokens)))  # returns number of unique tokens
ko.vocab()                  # returns frequency distribution

# 5. 챠트로 보기
ko.plot(50) #상위 50개의 unique token에 대해서 plot한결과를 아래와 같이 보여준다.

# 6. 특정 단어에 대해서 빈도수 확인하기
print(ko.count(str('스토리')))
print(ko.count(str('영상')))
print(ko.count(str('연출')))
print(ko.count(str('연기')))

# 7. 분산 차트 보기 (dispersion plot)
ko.dispersion_plot(['스토리','영상','연출','연기'])

# 8. 색인하기(본문속에서 해당 검색어가 포함된 문장을 찾아주는 것)
story_tmp = ko.concordance_list(('스토리'))
image_tmp = ko.concordance_list(('영상'))
direct_tmp = ko.concordance_list(('연출'))
act_tmp = ko.concordance_list(('연기'))

# 스토리 관련 문장을 추출한다.
str_story_tmp = ""
for i, v in enumerate(story_tmp):
    print("story_tmp[", i, "][4]", story_tmp[i][4])
    str_story_tmp = str_story_tmp + story_tmp[i][4]

str_story_tmp

# 영상 관련 문장을 추출한다.
str_image_tmp = ""
for i, v in enumerate(image_tmp):
    print("story_tmp[", i, "][4]", image_tmp[i][4])
    str_image_tmp = str_image_tmp + image_tmp[i][4]

str_image_tmp


# 연출 관련 문장을 추출한다.
str_direct_tmp = ""
for i, v in enumerate(direct_tmp):
    print("story_tmp[", i, "][4]", direct_tmp[i][4])
    str_direct_tmp = str_image_tmp + direct_tmp[i][4]

str_direct_tmp


# 연기 관련 문장을 추출한다.
str_act_tmp = ""
for i, v in enumerate(act_tmp):
    print("story_tmp[", i, "][4]", act_tmp[i][4])
    str_act_tmp = str_image_tmp + act_tmp[i][4]

str_act_tmp

predict_pos_neg(str_story_tmp)
predict_pos_neg(str_image_tmp)
predict_pos_neg(str_direct_tmp)
predict_pos_neg(str_act_tmp)
```

- movie_word2vec.py : Word2vec 라이브러리
```Console
# http://blog.naver.com/PostView.nhn?blogId=2feelus&logNo=220384206922&redirect=Dlog&widgetTypeCall=true
# [출처] 한글을 이용한 데이터마이닝및 word2vec이용한 유사도 분석|작성자 IDEO (참고하여 소스 커스터마이징)
# 1. 읽기
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import importlib
importlib.reload(sys)

from konlpy.corpus import kobill    # Docs from pokr.kr/bill
files_ko = kobill.fileids()         # Get file ids

# news.txt는 http://boilerpipe-web.appspot.com/ 를 통해 포탈뉴스 부분에서 긁어왔다.
# news.txt 는  konlpy의 corpus아래에 있는 kobill directory에 미리 저장되어있어야 한다.
# /Library/Python/2.7/site-packages/konlpy/data/corpus/kobill
doc_ko = kobill.open('ratings_train.txt').read()

# 2.Tokenize (의미단어 검출)
from konlpy.tag import Okt
import os
import json

# 학습시간이 오래 걸리므로 파일로 저장하여 처리 한다.
if os.path.isfile('tokens_ko_morphs.txt'):
    with open('tokens_ko_morphs.txt', encoding='UTF8') as f:
        tokens_ko_morphs = json.load(f)
else:
    okt = Okt()
    tokens_ko_morphs = okt.morphs(doc_ko)

    # JSON 파일로 저장
    with open('tokens_ko_morphs.txt', 'w', encoding="utf-8") as make_file:
        json.dump(tokens_ko_morphs, make_file, ensure_ascii=False, indent="\t")

# 3. Token Wapper 클래스 만들기(token에대해 이런 저런 처리를 하기 위해)
import nltk

# 일반적인 영화 평론의 기준과 유사한 단어를 추출한다.
# 스토리, 비주얼, 연출, 연기
ko = nltk.Text(tokens_ko_morphs, name='연기')   # 이름은 아무거나

# 4. 토근 정보및 단일 토큰 정보 알아내기
print(len(ko.tokens))       # returns number of tokens (document length)
print(len(set(ko.tokens)))  # returns number of unique tokens
ko.vocab()                  # returns frequency distribution

# 5. 챠트로 보기
ko.plot(50) #상위 50개의 unique token에 대해서 plot한결과를 아래와 같이 보여준다.

# 6. 특정 단어에 대해서 빈도수 확인하기
print(ko.count(str('연기')))

# 7. 분산 차트 보기 (dispersion plot)
ko.dispersion_plot(['스토리','영상','연출','연기'])

# 8. 색인하기(본문속에서 해당 검색어가 포함된 문장을 찾아주는 것)
ko.concordance(('스토리'))
ko.concordance(('영상'))
ko.concordance(('연출'))
ko.concordance(('연기'))

# 9. 유의어 찾기
ko.similar('스토리')
ko.similar('영상')
ko.similar('연출')
ko.similar('연기')

# 10. 의미단위로 나누기(Tagging and chunking)
# 10.1 형태소 분석기(POS tagging)
# 명사, 형용사, 조사등을 나누어 보여준다.
tags_ko = okt.pos('작고 노란 강아지가 고양이에게 짖었다')

print(tags_ko)

# 10.2 명사구단위로 묵어주기(Noun Phrase Chunking)
parser_ko = nltk.RegexpParser("NP: {<Adjective>*<Noun>*}")
chunks_ko = parser_ko.parse(tags_ko)

chunks_ko.draw()


# 3.1. Topic Modeling (LSI,LDA,HDP 알고리즘)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

from konlpy.corpus import kobill
docs_ko = [kobill.open(i).read() for i in kobill.fileids()]

from konlpy.tag import Twitter; t=Twitter()
pos = lambda d: ['/'.join(p) for p in t.pos(d, stem=True, norm=True)]
texts_ko = [pos(doc) for doc in docs_ko]

#encode tokens to integers
from gensim import corpora
dictionary_ko = corpora.Dictionary(texts_ko)
dictionary_ko.save('ko.dict')  # save dictionary to file for future use

#calulate TF-IDF
from gensim import models
tf_ko = [dictionary_ko.doc2bow(text) for text in texts_ko]
tfidf_model_ko = models.TfidfModel(tf_ko)
tfidf_ko = tfidf_model_ko[tf_ko]
corpora.MmCorpus.serialize('ko.mm', tfidf_ko) # save corpus to file for future use

#train topic model
#LSI
ntopics, nwords = 3, 5
lsi_ko = models.lsimodel.LsiModel(tfidf_ko, id2word=dictionary_ko, num_topics=ntopics)
print(lsi_ko.print_topics(num_topics=ntopics, num_words=nwords))

#LDA
import numpy as np; np.random.seed(42)  # optional
lda_ko = models.ldamodel.LdaModel(tfidf_ko, id2word=dictionary_ko, num_topics=ntopics)
print(lda_ko.print_topics(num_topics=ntopics, num_words=nwords))

#HDP
import numpy as np; np.random.seed(42)  # optional
hdp_ko = models.hdpmodel.HdpModel(tfidf_ko, id2word=dictionary_ko)
print(hdp_ko.print_topics(topics=ntopics, topn=nwords))

#Scoring document
bow = tfidf_model_ko[dictionary_ko.doc2bow(texts_ko[0])]
sorted(lsi_ko[bow], key=lambda x: x[1], reverse=True)
sorted(lda_ko[bow], key=lambda x: x[1], reverse=True)
sorted(hdp_ko[bow], key=lambda x: x[1], reverse=True)

bow = tfidf_model_ko[dictionary_ko.doc2bow(texts_ko[1])]
sorted(lsi_ko[bow], key=lambda x: x[1], reverse=True)
sorted(lda_ko[bow], key=lambda x: x[1], reverse=True)
sorted(hdp_ko[bow], key=lambda x: x[1], reverse=True)
```

- naver_movie_api.py : 네이버 영화 정보 크롤러
```Console
# -*- coding: utf-8 -*-
# 출처 http://airpage.org/xe/language_data/22881
from bs4 import BeautifulSoup
import urllib.request
from urllib.parse import quote
import json
import re
import requests
from datetime import datetime
from collections import OrderedDict

# 네이버 검색 Open API 사용 요청시 얻게되는 정보를 입력합니다
naver_client_id = "LwIv4iBS06sNbnFNjYSG"
naver_client_secret = "OVpKPW1ZUd"


def cleanhtml(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext


def searchByTitle(title):
    myurl = 'https://openapi.naver.com/v1/search/movie.json?display=100&query=' + quote(title)
    request = urllib.request.Request(myurl)
    request.add_header("X-Naver-Client-Id", naver_client_id)
    request.add_header("X-Naver-Client-Secret", naver_client_secret)
    response = urllib.request.urlopen(request)
    rescode = response.getcode()
    if (rescode == 200):
        response_body = response.read()
        d = json.loads(response_body.decode('utf-8'))
        if (len(d['items']) > 0):
            return d['items']
        else:
            return None

    else:
        print("Error Code:" + rescode)


def findItemByInput(items):
    movie_list = []

    for index, item in enumerate(items):
        navertitle = cleanhtml(item['title'])
        naversubtitle = cleanhtml(item['subtitle'])
        naverpubdate = cleanhtml(item['pubDate'])
        naveractor = cleanhtml(item['actor'])
        naverlink = cleanhtml(item['link'])
        naveruserScore = cleanhtml(item['userRating'])

        navertitle1 = navertitle.replace(" ", "")
        navertitle1 = navertitle1.replace("-", ",")
        navertitle1 = navertitle1.replace(":", ",")

        # 기자 평론가 평점을 얻어 옵니다
        spScore = getSpecialScore(naverlink)

        # 네이버가 다루는 영화 고유 ID를 얻어 옵니다다
        naverid = re.split("code=", naverlink)[1]

        # 영화의 타이틀 이미지를 표시합니다
        # if (item['image'] != None and "http" in item['image']):
        #    response = requests.get(item['image'])
        #    img = Image.open(BytesIO(response.content))
        #    img.show()

        # print(index, navertitle, naversubtitle, naveruserScore, spScore)

        movie_list.append(index)
        movie_list.append(navertitle)
        movie_list.append(naversubtitle)
        movie_list.append(naveruserScore)
        movie_list.append(spScore)

    # print(movie_list)

    return movie_list


def getInfoFromNaver(searchTitle):
    items = searchByTitle(searchTitle)
    movie_list_tmp = []

    if (items != None):
        movie_list_tmp = findItemByInput(items)
    else:
        print("No result")

    return movie_list_tmp

def get_soup(url):
    source_code = requests.get(url)
    plain_text = source_code.text
    soup = BeautifulSoup(plain_text, 'lxml')
    return soup


# 기자 평론가 평점을 얻어 옵니다
def getSpecialScore(URL):
    soup = get_soup(URL)
    scorearea = soup.find_all('div', "spc_score_area")
    newsoup = BeautifulSoup(str(scorearea), 'lxml')
    score = newsoup.find_all('em')
    if (score and len(score) > 5):
        scoreis = score[1].text + score[2].text + score[3].text + score[4].text
        return float(scoreis)
    else:
        return 0.0


def get_movie_review_data(url):
    movie_review_data = []
    resp = requests.get(url)
    html = BeautifulSoup(resp.content, 'html.parser')
    score_result = html.find('div', {'class': 'score_result'})
    lis = score_result.findAll('li')
    for li in lis:
        nickname = li.findAll('a')[0].find('span').getText()
        created_at = datetime.strptime(li.find('dt').findAll('em')[-1].getText(), "%Y.%m.%d %H:%M")

        review_text = li.find('p').getText()
        score = li.find('em').getText()
        btn_likes = li.find('div', {'class': 'btn_area'}).findAll('span')
        like = btn_likes[1].getText()
        dislike = btn_likes[3].getText()

        watch_movie = li.find('span', {'class': 'ico_viewer'})

        # 간단하게 프린트만 했습니다.
        # print(nickname, review_text, score, like, dislike, created_at, watch_movie and True or False)
        movie_review_data.append(nickname)
        movie_review_data.append(review_text)
        movie_review_data.append(score)
        movie_review_data.append(like)
        movie_review_data.append(dislike)
        movie_review_data.append(created_at)

    return movie_review_data

def get_movie_review(code):
    test_url = 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=' +code+ '&type=after'
    resp = requests.get(test_url)
    html = BeautifulSoup(resp.content, 'html.parser')
    result = html.find('div', {'class': 'score_total'}).find('strong').findChildren('em')[1].getText()
    total_count = int(result.replace(',', ''))

    # 너무 많다.. 조금만
    total_count = (total_count * 3) / 100

    movie_review_data = []
    for i in range(1, int(total_count / 10) + 1):
        url = test_url + '&page=' + str(i)
        print('url: "' + url + '" is parsing....')
        movie_review_data.append(get_movie_review_data(url))

    return(movie_review_data)

if __name__ == "__main__":
    print(getInfoFromNaver(u"The Mummy"))
    get_movie_review('136990')

```

- naver_movie_code_api.py : 네이버 영화 코드 크롤러
```Console
from bs4 import BeautifulSoup

import urllib.request
from urllib import parse

maximum = 0
maxpage = 40
maxpage_t =(int(maxpage)-1)*10+1

movie_code_list = []

def get_movie_code(title_tmp):
    page = 1

    while page <= maxpage_t:

        html = urllib.request.urlopen('https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=pnt&page=' + str(page))
        soup = BeautifulSoup(html, 'lxml')
        titles = soup.find_all('td', 'title')
        front_url = 'http://movie.naver.com/'

        for title in titles:
            url = parse.urlparse(front_url + title.find('a')['href'])
            key_tmp = parse.parse_qs(url.query)
            key_tmp['title'] = title.find('a').text

            if(key_tmp['title'] == title_tmp):
                # print(title_tmp, key_tmp['code'])
                return key_tmp['code']

            # print(key_tmp)
            movie_code_list.append(key_tmp)

            # matching = [s for s in movie_code_list if "그린 북" in s]

        page += 10

    return movie_code_list

if __name__ == "__main__":
    print(get_movie_code('여고괴담'))
```

- df_response_lib.py : Webhook 라이브러리
```Console
# 출처 : https://github.com/pragnakalp/dialogflow-webhook-response-libary-in-python/blob/master/df_response_lib.py
# Responses for Actions On Google
class actions_on_google_response():

    # class variable initializer initializer
    def __init__(self):
        self.platform = "ACTIONS_ON_GOOGLE"

    """
    Actions on Google Simple Response Builder
    @param name=display_text, type=list
    Sample example of display_text ex. [["Text to be displayed", "Text to  be spoken", True]]
    """
    def simple_response(self, responses):

        if len(responses) > 2:
            raise Exception(
                "Responses argument in simple response should have at most two elements only.")
        else:
            # a list to store the responses
            responses_json = []
            # iterate through the list of responses given by the user
            for response in responses:
                # if SSML = True, build the ssml response, else build textToSpeech
                # response[2] = SSML boolean
                if response[2]:
                    # response dictionary
                    response_dict = {
                        # text to be diplayed
                        "displayText": str(response[0]),
                        # ssml response to be spoken
                        "ssml": str(response[1])
                    }
                else:
                    response_dict = {
                        # text to be displayed
                        "displayText": str(response[0]),
                        # text to speech text
                        "textToSpeech": str(response[1])
                    }

                # add the response dict to the responses list
                responses_json.append(response_dict)

            # return the simple response JSON
            return {
                "platform": self.platform,
                "simpleResponses": {
                    "simpleResponses": responses_json
                }
            }

    """"
    Actions on Google Basic Card Builder
    @param title = string
    @param subtitle = string
    @param formattedText = string
    @param image = list [image_url, accessibility_text]
    @param buttons = list of [button_title, url_link]
    """
    def basic_card(self, title, subtitle="", formattedText="", image=None, buttons=None):
        # list to store buttons responses
        buttons_json = []
        if buttons is not None:
            # iterate through the buttons list
            for button in buttons:
                # add the buttons response to the buttons list
                buttons_json.append(
                    {
                        # title of the button
                        "title": button[0],
                        # url to be opened by the button
                        "openUriAction": {
                            "uri": button[1]
                        }
                    }
                )

            # return basic card JSON
            response = {
                "platform": self.platform,
                "basicCard": {
                    "title": title,
                    "subtitle": subtitle,
                    "formattedText": formattedText,
                    "buttons": buttons_json,
                    "image": {
                        "imageUri": image[0],
                        "accessibilityText": image[1]
                    }
                }
            }

        else:
            # return basic card JSON
            response = {
                "platform": self.platform,
                "basicCard": {
                    "title": title,
                    "subtitle": subtitle,
                    "formattedText": formattedText,
                    "image": {
                        "imageUri": image[0],
                        "accessibilityText": image[1]
                    }
                }
            }

        return response

    """
    Actions on Google List response
    @param list_title = string
    @param list_elements = list of list response items
    """
    def list_select(self, list_title, list_elements):
        # as per the actions on google response list items must be between 2 and 30
        if len(list_elements) > 30 or len(list_elements) < 2:
            raise Exception("List items must be two or less than 30.")
        else:
            # items list to store list elements
            items_list = []
            # iterate through the list elements list
            for list_element in list_elements:
                # append the items to the items_list
                items_list.append(
                    {
                        # title of the list item
                        "title": list_element[0],
                        # description of the list item
                        "description": list_element[1],
                        # info aabout the list item
                        "info": {
                            # key of the list items, key is used as user say string
                            "key": list_element[2][0],
                            # synonyms are the words that can be used as a value for the option when the user types instead of selecting from the list
                            "synonyms": list_element[2][1]
                        },
                        # list image
                        "image": {
                            # URL
                            "imageUri": list_element[3][0],
                            # accessibility text to be spoken
                            "accessibilityText": list_element[3][1]
                        }
                    }
                )

        # return the list response
        return {
            "platform": self.platform,
            "listSelect": {
                "title": list_title,
                "items": items_list
            }
        }

    """
    Actions on Google Suggestions chips resoponse
    @param suggestions = list of strings
    """
    def suggestion_chips(self, suggestions):
        # suggestions_json to store the suggestions JSON
        suggestions_json = []
        # iterate through the suggestions list
        for suggestion in suggestions:
            # append the suggestion to the suggestions_json list
            suggestions_json.append(
                {
                    # title text to be displayed in the chip
                    "title": str(suggestion)
                }
            )

        # return the suggestion chips response JSON
        return {
            "platform": self.platform,
            "suggestions": {
                "suggestions": suggestions_json
            }
        }

    """
    Actions on Google Linkout suggestions
    @param title = string
    @param url = string (a valid URL)
    """
    def link_out_suggestion(self, title, url):
        # title should not be null
        if title == "" or url == "":
            raise Exception(
                "Provide the title and URL for link out suggestion response.")
        else:
            # return the link out suggestion response
            return {
                "platform": self.platform,
                "linkOutSuggestion": {
                    "destinationName": str(title),
                    "uri": str(url)
                }
            }

# Responses for Facebook
class facebook_response():

    # class variable initializer initializer
    def __init__(self):
        self.platform = "FACEBOOK"

    def text_response(self, texts):
        # text should contain at least one string
        if len(texts) <= 0:
            raise Exception("Provide the text for the text response")
        else:
            # text_obj list for storing the text variations
            text_obj = []
            for text in texts:
                text_obj.append(str(text))

            # return the text response
            return {
                "text": {
                    "text": text_obj
                },
                "platform": self.platform
            }

    def quick_replies(self, title, quick_replies_list):
        if title == "":
            raise Exception("Title is required for basic card in facebook.")
        # quick_replies_list must contains at least one string
        elif len(quick_replies_list) <= 0:
            raise Exception(
                "Quick replies response must contain at least on text string.")
        else:
            # quick replies list to store the quick replie text
            quick_replies = []
            for quick_reply in quick_replies_list:
                # append to the list
                quick_replies.append(
                    str(quick_reply)
                )

            # return the response JSON
            return {
                "quickReplies": {
                    "title": str(title),
                    "quickReplies": quick_replies
                },
                "platform": self.platform
            }

    def image_response(self, url):
        # check url
        if url == "":
            raise Exception("URL in the image response is required.")
        else:
            # return the JSON response
            return {
                "image": {
                    "imageUri": str(url)
                },
                "platform": self.platform
            }

    def card_response(self, title, buttons):
        buttons_json = []
        for button in buttons:
            buttons_json.append(
                {
                    "text": str(button[0]),
                    "postback": str(button[1])
                }
            )

        # return the card
        return {
            "card": {
                "title": str(title),
                "buttons": buttons_json
            },
            "platform": self.platform
        }

# Responses for Telegram
class telegram_response():

    # class variable initializer initializer
    def __init__(self):
        self.platform = "TELEGRAM"

    def text_response(self, texts):
        # text should contain at least one string
        if len(texts) <= 0:
            raise Exception("Provide the text for the text response")
        else:
            # text_obj list for storing the text variations
            text_obj = []
            for text in texts:
                text_obj.append(str(text))

            # return the text response
            return {
                "text": {
                    "text": text_obj
                },
                "platform": self.platform
            }

    def quick_replies(self, title, quick_replies_list):
        if title == "":
            raise Exception("Title is required for basic card in facebook.")
        # quick_replies_list must contains at least one string
        elif len(quick_replies_list) <= 0:
            raise Exception(
                "Quick replies response must contain at least on text string.")
        else:
            # quick replies list to store the quick replie text
            quick_replies = []
            for quick_reply in quick_replies_list:
                # append to the list
                quick_replies.append(
                    str(quick_reply)
                )

            # return the response JSON
            return {
                "quickReplies": {
                    "title": str(title),
                    "quickReplies": quick_replies
                },
                "platform": self.platform
            }

    def image_response(self, url):
        # check url
        if url == "":
            raise Exception("URL in the image response is required.")
        else:
            # return the JSON response
            return {
                "image": {
                    "imageUri": str(url)
                },
                "platform": self.platform
            }

    def card_response(self, title, buttons):
        buttons_json = []
        for button in buttons:
            buttons_json.append(
                {
                    "text": str(button[0]),
                    "postback": str(button[1])
                }
            )

        return {
            "card": {
                "title": str(title),
                "buttons": buttons_json
            },
            "platform": self.platform
        }


# dialogflow fulfillment response
class fulfillment_response():

    def __init__(self):
        pass

    # fulfillment text builder
    # @param fulfillmentText = string
    def fulfillment_text(self, fulfillmentText):
        if fulfillmentText == "":
            raise Exception("Fulfillment text should not be empty.")
        else:
            return {
                "fulfillment_text": str(fulfillmentText)
            }

    # fulfillment messages builder
    # @param response_objects (AOG response, FB response, Telegram response)
    def fulfillment_messages(self, response_objects):
        if len(response_objects) <= 0:
            raise Exception(
                "Response objects must contain at least one response object.")
        else:
            return {
                "fulfillment_messages": response_objects
            }

    # dialogflow output contexts
    # @param session = dialogflow session id
    # @param contexts = context name (string)
    def output_contexts(self, session, contexts):
        contexts_json = []
        for context in contexts:
            contexts_json.append({
                "name": session + "/contexts/" + context[0],
                "lifespanCount": context[1],
                "parameters": context[2]
            })

        # return the output context json
        return {
            "output_contexts": contexts_json
        }

    # dialogflow followup event JSON
    # @param name = event name
    # @param parameters = key value pair of parameters to be passed
    def followup_event_input(self, name, parameters):
        return {
            "followup_event_input": {
                "name": str(name),
                "parameters": parameters
            }
        }

    # main response with fulfillment text and fulfillment messages
    # @param fulfillment_text = fulfillment_text JSON
    # @param fulfillment_messages = fulfillment_messages JSON
    # @param output_contexts = output_contexts JSON
    # @param followup_event_input = followup_event_input JSON
    def main_response(self, fulfillment_text, fulfillment_messages=None, output_contexts=None, followup_event_input=None):
        if followup_event_input is not None:
            if output_contexts is not None:
                if fulfillment_messages is not None:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "fulfillmentMessages": fulfillment_messages['fulfillment_messages'],
                        "outputContexts": output_contexts['output_contexts'],
                        "followupEventInput": followup_event_input['followup_event_input']
                    }
                else:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "outputContexts": output_contexts['output_contexts'],
                        "followupEventInput": followup_event_input['followup_event_input']
                    }
            else:
                if fulfillment_messages is not None:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "fulfillmentMessages": fulfillment_messages['fulfillment_messages'],
                        "followupEventInput": followup_event_input['followup_event_input']
                    }
                else:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "followupEventInput": followup_event_input['followup_event_input']
                    }
        else:
            if output_contexts is not None:
                if fulfillment_messages is not None:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "fulfillmentMessages": fulfillment_messages['fulfillment_messages'],
                        "outputContexts": output_contexts['output_contexts']
                    }
                else:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "outputContexts": output_contexts['output_contexts']
                    }
            else:
                if fulfillment_messages is not None:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text'],
                        "fulfillmentMessages": fulfillment_messages['fulfillment_messages']
                    }
                else:
                    response = {
                        "fulfillmentText": fulfillment_text['fulfillment_text']
                    }

        # return the main dialogflow response
        return response
```

#### <참고>
#### [Dialogflow]
https://medium.com/zenofai/creating-chatbot-using-python-flask-d6947d8ef805
https://ndb796.tistory.com/133
https://pusher.com/tutorials/chatbot-flask-dialogflow
https://www.pragnakalp.com/dialogflow-fulfillment-webhook-tutorial/
https://dialogflow.com/docs/samples
https://www.satishmpandey.tech/creating-a-webhook-for-dialogflow-agent-using-python/
#### [Webhook]
https://kswims.tistory.com/143
https://jm4488.tistory.com/57
https://developers.exlibrisgroup.com/alma/integrations/webhooks/
#### [파이썬]
https://wikidocs.net/book/1
https://wikidocs.net/book/110
#### [딥러닝]
https://wikidocs.net/book/587
#### [자연어처리]
https://wikidocs.net/book/2155
#### [데이터 셋]
Naver sentiment movie corpus : http://github.com/e9t/nsmc/
[Keras] KoNLPy를 이용한 한국어 영화 리뷰 감정 분석 - Cyc1am3n's Blog
맛집리뷰 감성분석 :: 빅데이터 : https://bluediary8.tistory.com/17
활용사례 - 공공데이터포털 : https://www.data.go.kr/useCase/1004460/exam.do
리뷰 분석을 위한 음식점 데이터 수집 - SEMANTICS : http://semantics.kr/%EB%A6%AC%EB%B7%B0-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%EC%9D%8C%EC%8B%9D%EC%A0%90-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91/
#### [크롤링]
나만의 웹 크롤러 만들기(3): Selenium으로 무적 크롤러 만들기 ... :  https://beomi.github.io/2017/02/27/HowToMakeWebCrawler-With-Selenium/
​
